{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dobby-Mphahlele/Problem-set-2/blob/main/Dobby_VoIP_Call_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pyspark"
      ],
      "metadata": {
        "id": "o--Hee9iMFpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "id": "cZq_2PcTLw01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Loading the data"
      ],
      "metadata": {
        "id": "at7J2LlXVqsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, unix_timestamp, sum as spark_sum, min as spark_min, max as spark_max\n",
        "\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"IPDR Analysis\").getOrCreate()\n",
        "\n",
        "# GitHub raw file URL\n",
        "url = 'https://raw.githubusercontent.com/Dobby-Mphahlele/Problem-set-2/1b478910cfb09c6768b5ebb490fe80c302d4b5ba/ipdr.csv'\n",
        "\n",
        "# Reading the CSV file into a Pandas DataFrame\n",
        "df_pd = pd.read_csv(url)\n",
        "\n",
        "# Converting Pandas DataFrame to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(df_pd)\n",
        "\n",
        "# Showing the schema and a few rows of the Spark DataFrame\n",
        "spark_df.printSchema()\n",
        "spark_df.show(5)\n"
      ],
      "metadata": {
        "id": "kO3zKjL-d6v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting datetime columns to timestamp type"
      ],
      "metadata": {
        "id": "XH0QErrTJY8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark_df = spark_df.withColumn(\"ST\", unix_timestamp(col(\"starttime\"), \"yyyy-MM-ddHH:mm:ss\").cast(\"timestamp\"))\n",
        "spark_df = spark_df.withColumn(\"ET\", unix_timestamp(col(\"endtime\"), \"yyyy-MM-ddHH:mm:ss\").cast(\"timestamp\"))\n",
        "\n",
        "# Show the updated DataFrame\n",
        "spark_df.dtypes\n"
      ],
      "metadata": {
        "id": "TxJ41EDgefun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting each MSISDN and specific start and end datetime domain/app wise:"
      ],
      "metadata": {
        "id": "-99NnOv3J4Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by MSISDN, domain, and VoIP APP to identify each call\n",
        "call_df = spark_df.groupBy(\"msisdn\", \"domain\").agg(\n",
        "    spark_min(\"starttime\").alias(\"First_ST\"),\n",
        "    spark_max(\"endtime\").alias(\"Last_ET\"),\n",
        "    spark_sum(\"dlvolume\").alias(\"Total_DL_Volume\"),\n",
        "    spark_sum(\"ulvolume\").alias(\"Total_UL_Volume\")\n",
        ")\n",
        "\n",
        "# Show the aggregated data\n",
        "call_df.show()\n"
      ],
      "metadata": {
        "id": "c4t12SSiewY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating ET(ET-10 min) for each FDR and handling idle time exclusion"
      ],
      "metadata": {
        "id": "gmCPElrgon_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr, col\n",
        "\n",
        "# Calculate ET - 10 minutes, handling nulls with COALESCE\n",
        "call_df = call_df.withColumn(\"ET_minus_10min\", expr(\"COALESCE(Last_ET - interval 10 minutes, Last_ET)\"))\n",
        "\n",
        "# If ET-10 min < ST, keep the original ET\n",
        "call_df = call_df.withColumn(\"Final_ET\", expr(\"CASE WHEN COALESCE(ET_minus_10min, Last_ET) < First_ST THEN Last_ET ELSE ET_minus_10min END\"))\n",
        "\n",
        "# Show the updated DataFrame\n",
        "call_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "J2h5KeUDf5Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating total volume of each call in Kb"
      ],
      "metadata": {
        "id": "XToqLT2OYfyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating total volume in Kb (since UL and DL volumes are in bytes)\n",
        "call_df = call_df.withColumn(\"Total_Volume_Kb\", (col(\"Total_DL_Volume\") + col(\"Total_UL_Volume\")) / 1024)\n",
        "\n",
        "# Registering ipdr as a temporary view\n",
        "call_df.createOrReplaceTempView('ipdr')\n",
        "# Showing the updated DataFrame\n",
        "call_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ibOPaUfTtPk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating total time of each call in seconds"
      ],
      "metadata": {
        "id": "0cGdDWl_YzSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import unix_timestamp, col\n",
        "\n",
        "# Calculate total time in seconds\n",
        "total_time_df = spark.sql(\"\"\"\n",
        "  SELECT\n",
        "    *,\n",
        "    (UNIX_TIMESTAMP(Last_ET, \"yyyy-MM-ddHH:mm:ss\") - UNIX_TIMESTAMP(First_ST, \"yyyy-MM-ddHH:mm:ss\")) AS Total_Time_Sec\n",
        "  FROM ipdr\n",
        "\"\"\")\n",
        "total_time_df.createOrReplaceTempView(\"volume_time_ipdr\")\n",
        "total_time_df.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PKIZpyxSvUSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating bit rate (kbps) of each call"
      ],
      "metadata": {
        "id": "cEomszsOI29r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bit_rate_df = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "      *,\n",
        "    ((Total_Volume_Kb) / (Total_Time_Sec)) * 1000  AS Bit_Rate_Kbps\n",
        "    FROM volume_time_ipdr\n",
        "  \"\"\"\n",
        ")\n",
        "bit_rate_df.createOrReplaceTempView(\"bit_rate_ipdr\")\n",
        "bit_rate_df.show()"
      ],
      "metadata": {
        "id": "bZJK0_Y0IeSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identification of Audio or Video call and its count"
      ],
      "metadata": {
        "id": "DyeTEn6DZFU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    *,\n",
        "    CASE\n",
        "        WHEN Bit_Rate_Kbps <= 200 THEN 'Yes'\n",
        "        ELSE 'No'\n",
        "    END AS isAudio,\n",
        "    CASE\n",
        "        WHEN Bit_Rate_Kbps > 200 THEN 'Yes'\n",
        "        ELSE 'No'\n",
        "    END AS isVideo\n",
        "FROM bit_rate_ipdr\n",
        "WHERE Bit_Rate_Kbps >= 10  --- Filtering out calls with bit rate < 10 kbps\n",
        "\"\"\"\n",
        "\n",
        ")\n",
        "result_df.show()"
      ],
      "metadata": {
        "id": "618svWxA8Ayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "piHUEQwlbC_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result_df.select(\"msisdn\", \"domain\", \"Total_Time_Sec\", \"Total_Volume_Kb\", \"Bit_Rate_kbps\", \"isAudio\", \"isVideo\")\n",
        "result_df.show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "hpc3PTvvZZ-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Output"
      ],
      "metadata": {
        "id": "juN_Wv3EM8Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registering final_df as a temporary view\n",
        "result_df.createOrReplaceTempView(\"result_df\")\n",
        "\n",
        "# Perform the aggregation using Spark SQL\n",
        "final_df = spark.sql(\"\"\"\n",
        "  SELECT\n",
        "    msisdn,\n",
        "    domain,\n",
        "    Total_Time_Sec,\n",
        "    Total_Volume_Kb,\n",
        "    Bit_Rate_kbps as kbps,\n",
        "    COUNT(*) OVER (PARTITION BY msisdn) AS fdr_count,\n",
        "    isAudio,\n",
        "    isVideo\n",
        "  FROM result_df\n",
        "  GROUP BY msisdn, domain, Total_Time_Sec, Total_Volume_Kb, Bit_Rate_kbps, isAudio, isVideo\n",
        "\"\"\")\n",
        "\n",
        "final_df.show(truncate=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xk5B7SjifcxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}